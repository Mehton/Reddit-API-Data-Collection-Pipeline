{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Climate Change Posts Data Collection\n",
        "\n",
        "This notebook downloads recent posts from the r/climate, r/environment, r/sustainability subreddit using the Reddit API (PRAW) and saves them to a CSV file.\n",
        "\n",
        "**Author**: Bhupinder jit Mehton                                                \n",
        "**Created**: November 2, 2025  \n",
        "**Purpose**: Collect climate change posts for analysis"
      ],
      "metadata": {
        "id": "GmjZ0oGXM51u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Required Libraries\n",
        "\n",
        "Import all necessary libraries for Reddit API access, CSV handling, and environment variable management."
      ],
      "metadata": {
        "id": "YvOzlCuMNiBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install praw python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-sflOfONnXe",
        "outputId": "9b290219-6a98-4159-e47e-2cc44ca73458"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.12/dist-packages (7.8.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.12/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.12/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.12/dist-packages (from praw) (1.9.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import praw\n",
        "import csv\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKKTPYdlNp_s",
        "outputId": "ff97392f-56e8-43d5-aa5d-12471c0bc858"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Environment Variables\n",
        "\n",
        "Load Reddit API credentials from the environment file."
      ],
      "metadata": {
        "id": "MQ61CKbfNxRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables from .env file\n",
        "# load_dotenv('reddit_api.env') # This line was commented out\n",
        "from dotenv import dotenv_values\n",
        "import os\n",
        "\n",
        "# Define the path to your .env file.\n",
        "\n",
        "env_file_path = '/content/reddit_api_template.env'\n",
        "\n",
        "\n",
        "# Load environment variables from reddit_api.env file if it exists\n",
        "if os.path.exists(env_file_path):\n",
        "    config = dotenv_values(env_file_path)\n",
        "    print(f\" Environment variables loaded from {env_file_path}!\")\n",
        "else:\n",
        "    config = {}\n",
        "    print(f\"Error: '{env_file_path}' not found. Environment variables not loaded.\")\n",
        "    print(\"Please ensure the 'reddit_api.env' file is in the specified Google Drive path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvoLYynaN5Ac",
        "outputId": "9e51852f-76c1-42c9-b937-ddce90fc93ba"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Environment variables loaded from /content/reddit_api_template.env!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Authenticate with Reddit API\n",
        "\n",
        "Establish connection to Reddit using PRAW with the loaded credentials."
      ],
      "metadata": {
        "id": "UMdwgNZeOL7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with Reddit using environment variables\n",
        "reddit = praw.Reddit(\n",
        "    client_id=config.get('REDDIT_CLIENT_ID'),\n",
        "    client_secret=config.get('REDDIT_CLIENT_SECRET'),\n",
        "    username=config.get('REDDIT_USERNAME'),\n",
        "    #password=config.get('REDDIT_PASSWORD'),\n",
        "    user_agent=config.get('REDDIT_USER_AGENT')\n",
        ")\n",
        "\n",
        "print(\"Reddit API authenticated successfully!\")\n",
        "print(f\"Connected as: {reddit.user.me()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNLOLiX0OTm2",
        "outputId": "6593a062-72fc-4195-c4bf-0d36cb7b0276"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reddit API authenticated successfully!\n",
            "Connected as: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define Data Collection Functions\n",
        "\n",
        "Create functions to download recent posts from a specified subreddit with proper error handling and improvements."
      ],
      "metadata": {
        "id": "U6PmEvBlOfk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipADxYacLVkO",
        "outputId": "6b55ce9a-978b-4804-f533-b02de48dbf29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 50 posts from r/climate.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 50 posts from r/environment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 50 posts from r/sustainability.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searched 'global warming' in r/climate: collected 50 posts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searched 'global warming' in r/environment: collected 50 posts.\n",
            "Searched 'global warming' in r/sustainability: collected 50 posts.\n",
            "\n",
            "Saved 300 unique posts to reddit_climate_data.csv.\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Helper function: Extract post attributes safely\n",
        "\n",
        "def extract_post_data(post, search_query=None):\n",
        "    \"\"\"Extracts relevant attributes from a Reddit post.\"\"\"\n",
        "    return {\n",
        "        \"title\": getattr(post, \"title\", None),\n",
        "        \"score\": getattr(post, \"score\", None),\n",
        "        \"upvote_ratio\": getattr(post, \"upvote_ratio\", None),\n",
        "        \"num_comments\": getattr(post, \"num_comments\", None),\n",
        "        \"author\": str(getattr(post, \"author\", None)),\n",
        "        \"subreddit\": str(getattr(post, \"subreddit\", None)),\n",
        "        \"url\": getattr(post, \"url\", None),\n",
        "        \"permalink\": f\"https://www.reddit.com{getattr(post, 'permalink', None)}\",\n",
        "        \"created_utc\": getattr(post, \"created_utc\", None),\n",
        "        \"is_self\": getattr(post, \"is_self\", None),\n",
        "        \"selftext\": (getattr(post, \"selftext\", \"\")[:500] if getattr(post, \"selftext\", None) else None),\n",
        "        \"flair\": getattr(post, \"link_flair_text\", None),\n",
        "        \"domain\": getattr(post, \"domain\", None),\n",
        "        \"search_query\": search_query\n",
        "    }\n",
        "\n",
        "\n",
        "# Fetch \"Hot\" Posts\n",
        "\n",
        "def fetch_hot_posts(subreddits, limit=50):\n",
        "    all_posts = []\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        hot_posts = subreddit.hot(limit=limit)\n",
        "\n",
        "        count = 0\n",
        "        for post in hot_posts:\n",
        "            all_posts.append(extract_post_data(post))\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Collected {count} posts from r/{subreddit_name}.\")\n",
        "\n",
        "    return all_posts\n",
        "\n",
        "\n",
        "# Keyword-Based Search\n",
        "\n",
        "def search_posts(query, subreddits, limit=50):\n",
        "    search_results = []\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        results = subreddit.search(query, limit=limit)\n",
        "\n",
        "        count = 0\n",
        "        for post in results:\n",
        "            search_results.append(extract_post_data(post, search_query=query))\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Searched '{query}' in r/{subreddit_name}: collected {count} posts.\")\n",
        "\n",
        "    return search_results\n",
        "\n",
        "\n",
        "# Data Export to CSV\n",
        "\n",
        "def export_to_csv(data, filename=\"reddit_data.csv\"):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.drop_duplicates(subset=[\"permalink\"], inplace=True)\n",
        "    df.replace({np.nan: None}, inplace=True)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nSaved {len(df)} unique posts to {filename}.\")\n",
        "\n",
        "\n",
        "# Run the collection pipeline\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target_subreddits = [\"climate\", \"environment\", \"sustainability\"]\n",
        "\n",
        "    # Fetch hot posts\n",
        "    hot_data = fetch_hot_posts(target_subreddits, limit=50)\n",
        "\n",
        "    # Search posts by keyword (example: \"global warming\")\n",
        "    keyword_data = search_posts(\"global warming\", target_subreddits, limit=50)\n",
        "\n",
        "    # Combine both datasets\n",
        "    all_data = hot_data + keyword_data\n",
        "\n",
        "    # Export to CSV\n",
        "    export_to_csv(all_data, \"reddit_climate_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Verify Results\n",
        "\n",
        "Check the created CSV file and display some sample data using pandas."
      ],
      "metadata": {
        "id": "A07Q7C-vUhqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check if file exists and load data\n",
        "if os.path.exists('/content/reddit_climate_data.csv'):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv('/content/reddit_climate_data.csv')\n",
        "\n",
        "    print(f\" Dataset Overview:\")\n",
        "    print(f\"   Total posts: {len(df)}\")\n",
        "    print(f\"   Columns: {list(df.columns)}\")\n",
        "    print(f\"   File size: {os.path.getsize('/content/reddit_climate_data.csv')} bytes\")\n",
        "\n",
        "    print(f\"\\n Sample Posts:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Display first 3 posts\n",
        "    for i, row in df.head(3).iterrows():\n",
        "        print(f\"\\nPost {i+1}:\")\n",
        "        print(f\"Title: {row['title'][:80]}...\")\n",
        "        print(f\"Author: {row['author']}\")\n",
        "        print(f\"Score: {row['score']}\")\n",
        "        print(f\"URL: {row['url']}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"\\n Basic Statistics:\")\n",
        "    print(f\"   Average score: {df['score'].mean():.2f}\")\n",
        "    print(f\"   Highest score: {df['score'].max()}\")\n",
        "    print(f\"   Lowest score: {df['score'].min()}\")\n",
        "\n",
        "else:\n",
        "    print(f\"File '{'/content/reddit_climate_data.csv'}' not found!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDOrS5arUjy9",
        "outputId": "a908df6d-6c47-4d6b-fb71-bbb96be38346"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset Overview:\n",
            "   Total posts: 300\n",
            "   Columns: ['title', 'score', 'upvote_ratio', 'num_comments', 'author', 'subreddit', 'url', 'permalink', 'created_utc', 'is_self', 'selftext', 'flair', 'domain', 'search_query']\n",
            "   File size: 138019 bytes\n",
            "\n",
            " Sample Posts:\n",
            "==================================================\n",
            "\n",
            "Post 1:\n",
            "Title: How to get involved with a local group to create the political will for climate ...\n",
            "Author: silence7\n",
            "Score: 1499\n",
            "URL: https://www.reddit.com/r/climate/comments/b49xgi/how_to_get_involved_with_a_local_group_to_create/\n",
            "------------------------------\n",
            "\n",
            "Post 2:\n",
            "Title: Bill Gates Says Climate Change ‘Will Not Lead to Humanity’s Demise’. In a memo, ...\n",
            "Author: coolbern\n",
            "Score: 208\n",
            "URL: https://www.nytimes.com/2025/10/28/climate/bill-gates-climate-change-humanity.html?unlocked_article_code=1.yE8.njbJ.jmwXlb3yEWw-&smid=url-share\n",
            "------------------------------\n",
            "\n",
            "Post 3:\n",
            "Title: US accused of ‘bully-boy’ tactics to sink climate deal | Officials say ‘threats’...\n",
            "Author: silence7\n",
            "Score: 396\n",
            "URL: https://www.ft.com/content/4e0a9a30-b014-4745-afe5-c841e36b41da?accessToken=zwAGQp38pV-4kc9OCpowsBRHRdOv5chB42tB2g.MEUCIAN8f6BfbZraSWocwrn0ZlnQbFC_jF5d7c79l8avDuxLAiEAziulAmVAhWfSCy8H53r5-8ppcZFAGj8RZGLS5Z9rjDs&sharetype=gift&token=db8b72ea-1385-4da6-94f1-95f776aad111\n",
            "------------------------------\n",
            "\n",
            " Basic Statistics:\n",
            "   Average score: 854.22\n",
            "   Highest score: 21919\n",
            "   Lowest score: 0\n"
          ]
        }
      ]
    }
  ]
}